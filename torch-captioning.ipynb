{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Apr  9 11:43:48 2021       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 450.66       Driver Version: 450.66       CUDA Version: 11.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  GeForce RTX 2070    Off  | 00000000:26:00.0  On |                  N/A |\r\n",
      "| 28%   34C    P0    56W / 215W |    979MiB /  7979MiB |     27%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A      1124      G   ...-xorg-server-1.20.8/bin/X      360MiB |\r\n",
      "|    0   N/A  N/A      1473      G   ...nt-system/sw/bin/kwin_x11      133MiB |\r\n",
      "|    0   N/A  N/A      1507      G   ...ce-5.18.5/bin/plasmashell       61MiB |\r\n",
      "|    0   N/A  N/A      1649      G   ...AAAAAAAAA= --shared-files      417MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchtext'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-21efe41d0c5f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbleu_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchtext'"
     ]
    }
   ],
   "source": [
    "# Packages we need\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import transformers\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.metrics import bleu_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Callable, Optional\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "## For Reproducibility\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything(42)\n",
    "\n",
    "## Tokenizer\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n",
    "\n",
    "## Device Configuration \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(\"Everything works\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths\n",
    "data_dir = './data/flickr30k_images'\n",
    "image_dir = f'{data_dir}/flickr30k_images'\n",
    "csv_file = f'{data_dir}/results.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's an error on line 19999 of this dataset, I had to search this up to fix it\n",
    "df = pd.read_csv(csv_file, delimiter='|')\n",
    "df[' comment_number'][19999] = ' 4'\n",
    "df[' comment'][19999] = ' A dog runs across the grass .'\n",
    "df['image_name'] = image_dir+'/'+df['image_name']\n",
    "df.head(5)\n",
    "\n",
    "# Sort the data into a data frame with 4 comment cells on each row\n",
    "image_name = {\n",
    "    'image_name':df[df[' comment_number'] == df[' comment_number'][0]]['image_name'].values,\n",
    "}\n",
    "comments = {\n",
    "    'comment_0':df[df[' comment_number'] == df[' comment_number'][0]][' comment'].values,\n",
    "    'comment_1':df[df[' comment_number'] == df[' comment_number'][1]][' comment'].values,\n",
    "    'comment_2':df[df[' comment_number'] == df[' comment_number'][2]][' comment'].values,\n",
    "    'comment_3':df[df[' comment_number'] == df[' comment_number'][3]][' comment'].values,\n",
    "    'comment_4':df[df[' comment_number'] == df[' comment_number'][4]][' comment'].values,\n",
    "}\n",
    "\n",
    "image_name_df = pd.DataFrame.from_dict(image_name)\n",
    "comments_df = pd.DataFrame.from_dict(comments)\n",
    "\n",
    "df = pd.concat([image_name_df,comments_df], axis=1)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Training and Test splits \n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "## Reset Indexes \n",
    "train = train.reset_index(drop=True)\n",
    "test = test.reset_index(drop=True)\n",
    "\n",
    "## Split training into training and validation \n",
    "train, val = train_test_split(train, test_size=0.25, random_state=42)\n",
    "\n",
    "## Reset Indexes \n",
    "train = train.reset_index(drop=True)\n",
    "val = val.reset_index(drop=True)\n",
    "\n",
    "## Get sizes\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, data, \n",
    "                 transforms: Optional[Callable] = None) -> None:\n",
    "        self.data = data\n",
    "        self.transforms = T.Compose([\n",
    "            T.Resize((256,256)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean = [0.5], std = [0.5]),\n",
    "        ])\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, i: int):\n",
    "        image_name = self.data.image_name.values[i]\n",
    "        image = Image.open(image_name).convert('RGB')\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image)\n",
    "            \n",
    "        comments = self.data[self.data.image_name == image_name].values.tolist()[0][1:]\n",
    "        encoded_inputs = tokenizer(comments,\n",
    "                            return_token_type_ids = False, \n",
    "                            return_attention_mask = False, \n",
    "                            max_length = 100, \n",
    "                            padding = \"max_length\",\n",
    "                            return_tensors = \"pt\")\n",
    "        \n",
    "        sample = {\"image\":image.to(device),\n",
    "                  \"captions\": encoded_inputs[\"input_ids\"].flatten().to(device)}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because our dataset has an odd number of files, drop_last ensures we don't get errors\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = FlickrDataset(train, transforms = True)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = batch_size, drop_last=True)\n",
    "\n",
    "val_dataset = FlickrDataset(val, transforms = True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = batch_size,drop_last=True)\n",
    "\n",
    "test_dataset = FlickrDataset(test, transforms = True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = batch_size,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a random image and its comment\n",
    "plt.imshow(train_dataset[42]['image'].permute(1,2,0).cpu())\n",
    "print(tokenizer.decode(train_dataset[42]['captions'].cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CNN - Based on ResNet\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        model = models.resnet50(pretrained=True)\n",
    "        # Freeze the resnet\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        \n",
    "        # Replace the output with our own embedding\n",
    "        modules = list(model.children())[:-1]\n",
    "        self.model = nn.Sequential(*modules)\n",
    "        self.embed = nn.Linear(model.fc.in_features * 4, embed_size)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        features = self.model(image)\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.embed(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The RNN\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size,\n",
    "                embedding_dim, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings = vocab_size,\n",
    "                                     embedding_dim = embedding_dim)\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size = input_size,\n",
    "                           hidden_size = hidden_size,\n",
    "                           batch_first = True)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def init_hidden(self, features):\n",
    "        # Intialize the hidden state, similar to an earlier lab\n",
    "        return (torch.autograd.Variable(torch.zeros(1,batch_size, self.hidden_size).to(device)),\n",
    "               torch.autograd.Variable(features.unsqueeze(0)).to(device))\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        state = self.init_hidden(features)\n",
    "        embed = self.embedding(captions)\n",
    "        lstm_out, state = self.lstm(embed, state)\n",
    "        outputs = self.fc(lstm_out)\n",
    "        outputs = outputs.view(-1, self.vocab_size)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size = 90000\n",
    "steps_per_epoch = 19069 // 32\n",
    "\n",
    "encoder = Encoder(embed_size = 512).to(device)\n",
    "decoder = Decoder(input_size = 512, \n",
    "                  hidden_size = 512, \n",
    "                  embedding_dim = 512, \n",
    "                  vocab_size = vocab_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters())\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(output, truth):\n",
    "    # TODO: Write a good accuracy function based on BLEU\n",
    "    x = tokenizer.decode(x.cpu())\n",
    "    y = tokenizer.decode(truth.cpu())\n",
    "    return bleu_score([x], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "train_accs = []\n",
    "val_losses = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(10):\n",
    "    #pbar = tqdm(total=len(train_loader))\n",
    "\n",
    "    for i, sample in enumerate(train_loader):\n",
    "        if i > steps_per_epoch:\n",
    "            break\n",
    "        \n",
    "        # Get the info\n",
    "        image, captions = sample['image'], sample['captions']\n",
    "        \n",
    "        # Zero grad\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Forward\n",
    "        features = encoder(image)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Loss\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        train_losses.append(loss.item())\n",
    "        train_accs.append(accuracy(outputs, captions))\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
